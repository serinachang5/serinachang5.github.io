<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS 294-286: Machine Learning & Human Behavior</title>
    <link href="https://fonts.googleapis.com/css?family=Montserrat:700,400|Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Helvetica', Arial, sans-serif;
            margin: 0;
            background: #f8fafc;
            color: #222;
            line-height: 1.7;
        }
        .container {
            max-width: 900px;
            margin: 40px auto 40px auto;
            background: #fff;
            border-radius: 16px;
            box-shadow: 0 4px 24px rgba(44,62,80,0.08);
            padding: 40px 32px 32px 32px;
        }
        h1, h2 {
            font-family: 'Montserrat', Arial, sans-serif;
            color: #2c3e50;
            margin-top: 0;
        }
        h1 {
            font-size: 2em;
            letter-spacing: 1px;
            margin-bottom: 0.2em;
        }
        h2 {
            font-size: 1.5em;
            margin-top: 2em;
            margin-bottom: 0.7em;
            border-bottom: 2px solid #e1e7ec;
            padding-bottom: 0.2em;
        }
        .header {
            text-align: center;
            margin-bottom: 2em;
        }
        .header p {
            margin: 0.3em 0;
            font-size: 1.08em;
        }
        .header strong {
            color: #34495e;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-top: 1.5em;
            background: #f9fbfc;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(44,62,80,0.04);
        }
        th, td {
            border: none;
            padding: 12px 10px;
            text-align: left;
            vertical-align: top;
            font-size: 1em;
        }
        th {
            background-color: #e1e7ec;
            color: #2c3e50;
            font-weight: 700;
            font-size: 1.08em;
            letter-spacing: 0.5px;
        }
        tr:nth-child(even) td {
            background-color: #f4f7fa;
        }
        tr td[colspan="2"] {
            background: linear-gradient(90deg, #e1e7ec 60%, #f9fbfc 100%);
            font-weight: bold;
            color: #34495e;
            font-size: 1.07em;
            letter-spacing: 0.5px;
            border-top: 2px solid #d1dbe6;
        }
        a {
            color: #2980b9;
            text-decoration: none;
            transition: color 0.2s;
        }
        a:hover {
            color: #1abc9c;
            text-decoration: underline;
        }
        em {
            color: #888;
        }
        @media (max-width: 700px) {
            .container {
                padding: 16px 6px;
            }
            h1 {
                font-size: 1.5em;
            }
            h2 {
                font-size: 1.1em;
            }
            table, th, td {
                font-size: 0.95em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>CS 294-286: Machine Learning & Human Behavior</h1>
            <p><strong>University of California, Berkeley | Fall 2025</strong></p>
            <p><strong>Instructor:</strong> <a href="https://serinachang5.github.io/" target="_blank">Serina Chang</a> (<a href="mailto:serinac@berkeley.edu">serinac@berkeley.edu</a>)</p>
            <p><strong>Time:</strong> TuTh 2:00-3:30pm</p>
            <p><strong>Location:</strong> Wheeler 200</p>
            <p><strong>Office Hours:</strong> By appointment</p>
            <!-- <div style="margin-top: 1.5em;">
                <p style="background: #ffe066; color: #b36b00; font-weight: bold; border-radius: 8px; padding: 12px 16px; font-size: 1.08em;">
                    <strong>If you are interested in taking this course and can't directly enroll, please fill out <a href="https://forms.gle/8BzLx91bC8Dg3GY7A" target="_blank" style="color: #d35400; text-decoration: underline;">this form</a></strong>.
                </p>
            </div> -->
        </div>

        <h2>Summary</h2>
        <p>This course will explore the intersection of machine learning (ML) and human behavior. The format of the course will be a mix of paper presentations, lectures, and a final project. We will cover three units:</p>

        <p><strong>Unit I: Modeling Human Behavior with ML.</strong>
            Predicting or simulating human behaviors is useful when behaviors are difficult to observe (e.g., for cost or privacy reasons) or cannot be observed (e.g., future or counterfactual behaviors). In this unit, we'll begin with recent efforts to simulate behaviors with LLMs, including survey responses, experimental results, and social interactions. We will discuss challenges in this domain, such as bias, diversity, validation, generalization, and scalability, along with approaches to address these challenges. We will start from the individual-level and scale up to entire networks and societies, exploring both LLM-based and more traditional approaches to modeling human societies.
        </p>

        <p><strong>Unit II: Algorithmically Infused Societies.</strong>
            In this unit, we'll discuss the interplay between algorithms and social systems: how algorithms shape human behaviors and human behaviors feed back into algorithms, resulting in feedback loops. We will study these loops in different contexts, such as recommender systems and social media feeds, and explore human awareness of algorithms and strategic shifts in behavior. We will end with a discussion about automating human decision-making, including how to compare human vs. algorithmic decisions under selective labels and the validity and risks of such automation.
        </p>

        <p><strong>Unit III: Adapting AI to Human Behavior.</strong>
            In this final unit, we will focus on how humans behave in interaction with AI systems and how we should adapt generative AI systems to work more effectively with humans. We will begin with analyses of human-AI interactions in the wild and discuss how to evaluate human-AI interactions. We will then explore various ways in which AI needs to adapt to humans in order to improve human-AI outcomes, such as how to understand human intents (e.g., given ambiguous user queries), learn individual preferences and personalize models, mitigate overreliance and provide explanations, and strive for complementarity.
        </p>

        <h2>Schedule</h2>
        <!-- <p>The schedule is tentative and will be finalized after the first class. More readings will be added soon.</p> -->
        <table>
            <tr>
                <th style="width: 100px;">Date</th>
                <th>Topic</th>
            </tr>
            <tr><td>8/28 Thu</td><td>
                Introduction to course
            </td></tr>
            <tr><td colspan="2"><strong>Unit I. Modeling Human Behaviors with ML</strong></td></tr>
            <tr><td>09/02 Tue</td><td>
                Introduction to LLM social simulation and agent-based modeling<br>
            </td></tr>
            <tr><td>09/04 Thu</td><td>
                Bias and diversity
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Santurkar et al (ICML 2023): 
                        <a href="https://proceedings.mlr.press/v202/santurkar23a" target="_blank">"Whose Opinions Do Language Models Reflect?"</a>
                    </li>
                    <li>
                        Bisbee et al (<i>Political Analysis</i> 2024): 
                        <a href="https://www.cambridge.org/core/journals/political-analysis/article/synthetic-replacements-for-human-survey-data-the-perils-of-large-language-models/B92267DC26195C7F36E63EA04A47D2FE" target="_blank">"Synthetic Replacements for Human Survey Data? The Perils of Large Language Models"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/09 Tue</td><td>
                Steerability: fine-tuning, steering vectors
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Chu et al (arXiv 2023): 
                        <a href="https://arxiv.org/abs/2303.16779" target="_blank">"Language Models Trained on Media Diets Can Predict Public Opinion"</a>
                    </li>
                    <li>
                        Kim et al (ICLR 2025): 
                        <a href="https://openreview.net/forum?id=rwqShzb9li" target="_blank">"Linear Representations of Political Perspective Emerge in Large Language Models"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/11 Thu</td><td>
                Validation and generalization
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Hewitt et al (preprint 2024): 
                        <a href="https://docsend.com/view/qeeccuggec56k9hd" target="_blank">"Predicting results of social science experiments using large language models"</a>
                    </li>
                    <li>
                        Binz et al (<i>Nature</i> 2025): 
                        <a href="https://www.nature.com/articles/s41586-025-09215-4" target="_blank">"A foundation model to predict and capture human cognition"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/16 Tue</td><td>
                <strong>GUEST LECTURE:</strong> <a href="https://www.joonsungpark.com/" target="_blank">Joon Sung Park</a><br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Park et al (UIST 2023): 
                        <a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606763" target="_blank">"Generative Agents: Interactive Simulacra of Human Behavior"</a>
                    </li>
                    <li>
                        Park et al (arXiv 2024): 
                        <a href="https://arxiv.org/abs/2411.10109" target="_blank">"Generative Agent Simulations of 1,000 People"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/18 Thu</td><td>
                Limits to prediction
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Salganik et al (<i>PNAS</i> 2020): 
                        <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1915006117" target="_blank">"Measuring the predictability of life outcomes with a scientific mass collaboration"</a>
                    </li>
                    <li>
                        Lundberg et al (<i>PNAS</i> 2024): 
                        <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2322973121" target="_blank">"The origins of unpredictability in life outcome prediction tasks"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/23 Tue</td><td>
                Social interactions and networks
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Zhou et al (ICLR 2024): 
                        <a href="https://openreview.net/forum?id=mM7VurbA4r" target="_blank">"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents"</a>
                    </li>
                    <li>
                        Jia et al (NeurIPS 2024): 
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1cb57fcf7ff3f6d37eebae5becc9ea6d-Paper-Conference.pdf" target="_blank">"Can Large Language Model Agents Simulate Human Trust Behavior?"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>09/25 Thu</td><td>Agent-based modeling and policy-planning
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Aleta et al (<i>Nature Hum Behav</i> 2020):
                        <a href="https://www.nature.com/articles/s41562-020-0931-9" target="_blank">"Modelling the impact of testing, contact tracing and household quarantine on second waves of COVID-19"</a>
                    </li>
                    <li>
                        Chopra et al (AAMAS 2025): 
                        <a href="https://dl.acm.org/doi/10.5555/3709347.3743565" target="_blank">"On the Limits of Agency in Agent-based Models"</a>
                    </li>
                </ul>
            </td></tr>

            <tr><td colspan="2"><strong>Unit II. Algorithmically Infused Societies</strong></td></tr>
            <tr><td>09/30 Tue</td><td>
                Recommendation systems
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Su et al (WWW 2016):
                        <a href="https://dl.acm.org/doi/10.1145/2872427.2883040" target="_blank">"The Effect of Recommendations on Network Structure"</a>
                    </li>
                    <li>
                        Chaney et al (RecSys 2018): 
                        <a href="https://dl.acm.org/doi/10.1145/3240323.3240370" target="_blank">"How algorithmic confounding in recommendation systems increases homogeneity and decreases utility"</a>
                    </li>
                </ul>
                Also see Wagner et al (<i>Nature</i> 2021): <a href="https://www.nature.com/articles/s41586-021-03666-1" target="_blank">"Measuring algorithmically infused societies"</a>
            </td></tr>
            <tr><td>10/02 Thu</td><td>
                Social media algorithms<br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Huszár et al (<i>PNAS</i> 2021):
                        <a href="https://www.pnas.org/doi/10.1073/pnas.2025334119" target="_blank">"Algorithmic amplification of politics on Twitter"</a>
                    </li>
                    <li>
                        Guess et al (<i>Science</i> 2023): 
                        <a href="https://www.science.org/doi/10.1126/science.abp9364" target="_blank">"How do social media feed algorithms affect attitudes and behavior in an election campaign?"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/07 Tue</td><td>
                <strong>GUEST LECTURE:</strong> <a href="http://jonathanstray.com/" target="_blank">Jonathan Stray</a><br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Thorburn et al (<i>Medium</i> 2023):
                        <a href="https://medium.com/understanding-recommenders/when-you-hear-filter-bubble-echo-chamber-or-rabbit-hole-think-feedback-loop-7d1c8733d5c" target="_blank">"When You Hear 'Filter Bubble', 'Echo Chamber', or 'Rabbit Hole' — Think 'Feedback Loop'"</a>
                    </li>
                    <li>
                        Stray et al (<i>Knight First Amendment Institute</i> 2023): 
                        <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429558" target="_blank">"The Algorithmic Management of Polarization and Violence on Social Media"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/09 Thu</td><td>Algorithmic awareness & strategic behavior
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Eslami et al (CHI 2016):
                        <a href="https://dl.acm.org/doi/10.1145/2858036.2858494" target="_blank">"First I 'like' it, then I hide it: Folk Theories of Social Feeds"</a>
                    </li>
                    <li>
                        Hron et al (ICLR 2023): 
                        <a href="https://openreview.net/forum?id=l6CpxixmUg" target="_blank">"Modeling content creator incentives on algorithm-curated platforms"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/14 Tue</td><td>Algorithms for human decision-making I
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Lakkaraju et al (KDD 2017):
                        <a href="https://dl.acm.org/doi/10.1145/3097983.3098066" target="_blank">"The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables"</a>
                    </li>
                    <li>
                        Perdomo et al (ICML 2020): 
                        <a href="https://proceedings.mlr.press/v119/perdomo20a.html" target="_blank">"Performative Prediction"</a>
                    </li>
                </ul>
                Also see Kleinberg et al (<i>QJE</i> 2018): <a href="https://academic.oup.com/qje/article-abstract/133/1/237/4095198" target="_blank">"Human Decisions and Machine Predictions"</a>
            </td></tr>
            <tr><td>10/16 Thu</td><td>Algorithms for human decision-making II
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Kleinberg et al (<i>American Economic Review</i> 2015):
                        <a href="https://www.aeaweb.org/articles?id=10.1257/aer.p20151023" target="_blank">"Prediction Policy Problems"</a>
                    </li>
                    <li>
                        Wang et al (<i>ACM Journal on Responsible Computing</i> 2024): 
                        <a href="https://dl.acm.org/doi/10.1145/3636509" target="_blank">"Against Predictive Optimization: On the Legitimacy of Decision-making Algorithms That Optimize Predictive Accuracy"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/21 Tue</td><td>
                <strong>GUEST LECTURE:</strong> <a href="https://diagdavenport.com/" target="_blank">Diag Davenport</a><br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Agan et al (<i>NBER</i> working paper, 2023):
                        <a href="https://www.nber.org/system/files/working_papers/w30981/w30981.pdf" target="_blank">"Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias"</a>
                    </li>
                </ul>
            </td></tr>
            
            <tr><td colspan="2"><strong>Unit III. Adapting AI to Human Behavior</strong></td></tr>
            <tr><td>10/23 Thu</td><td>Human-AI interactions in the wild
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Zhao et al (ICLR 2024):
                        <a href="https://openreview.net/forum?id=Bl8u7ZRlbM" target="_blank">"WildChat: 1M ChatGPT Interaction Logs in the Wild"</a>
                    </li>
                    <li>
                        Handa et al (arXiv 2025): 
                        <a href="https://arxiv.org/abs/2503.04761" target="_blank">"Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/28 Tue</td><td>Evaluating human-AI interactions
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Lee et al (<i>TMLR</i> 2023):
                        <a href="https://openreview.net/forum?id=hjDYJUn9l1" target="_blank">"Evaluating Human-Language Model Interaction"</a>
                    </li>
                    <li>
                        Collins et al (<i>PNAS</i> 2024): 
                        <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2318124121" target="_blank">"Evaluating language models for mathematics through interactions"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>10/30 Thu</td><td>Understanding human intents
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Shaikh et al (ACL 2025):
                        <a href="https://aclanthology.org/2025.acl-long.1016/" target="_blank">"Navigating Rifts in Human-LLM Grounding: Study and Benchmark"</a>
                    </li>
                    <li>
                        Wu et al (ICML 2025): 
                        <a href="https://openreview.net/forum?id=DmH4HHVb3y" target="_blank">"CollabLLM: From Passive Responders to Active Collaborators"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>11/04 Tue</td><td>Personalization, learning individual preferences
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Poddar et al (NeurIPS 2024):
                        <a href="https://neurips.cc/virtual/2024/poster/94141" target="_blank">"Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"</a>
                    </li>
                    <li>
                        Kirk et al (<i>Nature Mach Intell</i> 2024): 
                        <a href="https://www.nature.com/articles/s42256-024-00820-y" target="_blank">"The benefits, risks and bounds of personalizing the alignment of large language models to individuals"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>11/06 Thu</td><td>
                <strong>GUEST LECTURE:</strong> <a href="https://collinskatie.github.io/" target="_blank">Katie Collins</a><br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Collins et al (<i>Nature Hum Behav</i> 2024):
                        <a href="https://www.nature.com/articles/s41562-024-01991-9" target="_blank">"Building machines that learn and think with people"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>11/11 Tue</td><td><em>Academic and Administrative Holiday</em></td></tr>
            <tr><td>11/13 Thu</td><td>Human-AI complementarity I
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Vaccaro et al (<i>Nature Hum Behav</i> 2024):
                        <a href="https://www.nature.com/articles/s41562-024-02024-1" target="_blank">"When combinations of humans and AI are useful: A systematic review and meta-analysis"</a>
                    </li>
                    <li>
                        Zhang et al (FAccT 2020): 
                        <a href="https://dl.acm.org/doi/10.1145/3351095.3372852" target="_blank">"Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>11/18 Tue</td><td>Human-AI complementarity II
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Wilder et al (IJCAI 2020):
                        <a href="https://dl.acm.org/doi/abs/10.5555/3491440.3491652" target="_blank">"Learning to complement humans"</a>
                    </li>
                    <li>
                        Mozannar et al (NeurIPS 2023): 
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/61355b9c218505505d1bedede9da56b2-Paper-Conference.pdf" target="_blank">"Effective Human-AI Teams via Learned Natural Language Rules and Onboarding"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>11/20 Thu</td><td>Project presentations</td></tr>
            <tr><td>11/25 Tue</td><td>Project presentations</td></tr>
            <tr><td>11/27 Thu</td><td><em>Thanksgiving Break</em></td></tr>
            <tr><td>12/02 Tue</td><td>
                <strong>GUEST LECTURE:</strong> <a href="https://jessylin.com/" target="_blank">Jessy Lin</a><br>
                <ul style="font-size:0.97em; margin:0.5em 0 0.5em 1.2em; padding-left:1em;">
                    <li>
                        Lin et al (<i>TACL</i> 2024):
                        <a href="https://aclanthology.org/2024.tacl-1.50/" target="_blank">"Decision-Oriented Dialogue for Human-AI Collaboration"</a>
                    </li>
                </ul>
            </td></tr>
            <tr><td>12/04 Thu</td><td>Course wrap-up</td></tr>
        </table>
    
    <h2>Acknowledgements</h2>
    <p>
        Course topics were inspired in part by Johan Ugander's course on <a href="https://msande231.github.io/" target="_blank">Social Algorithms</a> and Joon Sung Park's course on <a href="https://joonspk-research.github.io/cs222-fall24/index.html" target="_blank">AI Agents and Simulations</a>. The course form was adapted from Sewon Min's course on 
        <a href="https://www.sewonmin.com/courses/cs294_288/" target="_blank">Data-Centric Large Language Models</a>.
    </p>
    </div>
</body>
</html>
